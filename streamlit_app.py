import streamlit as st
import torch
from PIL import Image
import cv2
import numpy as np
from ultralytics import YOLO
from torchvision import transforms
import torchvision.models as models

# ==================== ì„¤ì • ====================
# ëª¨ë¸ ë¡œë“œ
yolo_model = YOLO('first_model_best.pt')  # ì¬ì§ˆ ë¶„ë¥˜ìš©
# ì†ì„± ë¶„ë¥˜ ëª¨ë¸ (ë¹„ì›Œë„ ì‘ë™í•˜ë„ë¡ ì²˜ë¦¬)
cnn_model = None  # ì¶”í›„ torch.load('cnn_model.pth') ê°€ëŠ¥

# ì†ì„± í´ë˜ìŠ¤
attribute_names = ['broken', 'contaminated', 'contains_liquid',
                   'has_pattern', 'has_plastic_cap', 'has_metal_cap',
                   'has_plastic_label']

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

# Rule ê¸°ë°˜ ì¬í™œìš© íŒë‹¨
def predict_rule(material, attributes):
    if material == 'plastic_pet' and 'contaminated' in attributes:
        return "ì¬í™œìš© ë¶ˆê°€", "ì˜¤ì—¼ëœ í˜íŠ¸ë³‘ì€ ì¬í™œìš©ì´ ì–´ë ¤ì›€"
    if material == 'glass' and 'broken' not in attributes:
        return "ì¬í™œìš© ê°€ëŠ¥", "ê¹¨ë—í•˜ê³  ì†ìƒë˜ì§€ ì•Šì€ ìœ ë¦¬ëŠ” ì¬í™œìš© ê°€ëŠ¥"
    return "ì¬í™œìš© ë¶ˆê°€", "ì¬ì§ˆ ë˜ëŠ” ì†ì„± ì¡°ê±´ ë¶ˆì¶©ë¶„"


# ==================== UI ì‹œì‘ ====================
st.set_page_config(page_title="ì¬í™œìš© ì—¬ë¶€ íŒë‹¨ AI", layout="centered")
st.title("â™»ï¸ ì¬í™œìš© ì—¬ë¶€ íŒë‹¨ AI")

# ì‚¬ì´ë“œë°” ë©”ë‰´
option = st.sidebar.selectbox("ì…ë ¥ ë°©ì‹ ì„ íƒ", ("ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ì¹´ë©”ë¼ ì´¬ì˜"))

image = None

# ì´ë¯¸ì§€ ì—…ë¡œë“œ ë°©ì‹
if option == "ì´ë¯¸ì§€ ì—…ë¡œë“œ":
    uploaded_file = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "png", "jpeg"])
    if uploaded_file:
        image = Image.open(uploaded_file).convert("RGB")

# ì¹´ë©”ë¼ ì´¬ì˜ ë°©ì‹
elif option == "ì¹´ë©”ë¼ ì´¬ì˜":
    camera_image = st.camera_input("ì¹´ë©”ë¼ë¡œ ì‚¬ì§„ì„ ì°ì–´ì£¼ì„¸ìš”")
    if camera_image:
        image = Image.open(camera_image).convert("RGB")

# ==================== ì˜ˆì¸¡ ìˆ˜í–‰ ====================

if image:
    st.image(image, caption="ì…ë ¥ ì´ë¯¸ì§€", use_container_width=True)

    img_array = np.array(image)
    results = yolo_model(img_array)
    boxes = results[0].boxes.xyxy.cpu().numpy()
    cls_ids = results[0].boxes.cls.cpu().numpy()

    results_list = []

    for i, box in enumerate(boxes):
        x1, y1, x2, y2 = map(int, box)
        cropped = image.crop((x1, y1, x2, y2))
        input_tensor = transform(cropped).unsqueeze(0)

        # ì†ì„± ì˜ˆì¸¡ (ë¹„ì›Œë„ ì‹¤í–‰ë¨)
        if cnn_model:
            with torch.no_grad():
                attr_logits = cnn_model(input_tensor)
                attr_preds = torch.sigmoid(attr_logits).squeeze() > 0.5
                attr_labels = [attr for idx, attr in enumerate(attribute_names) if attr_preds[idx]]
        else:
            attr_labels = []  # ì†ì„± ì˜ˆì¸¡ ëª¨ë¸ ì—†ì„ ê²½ìš°

        material_class = results[0].names[int(cls_ids[i])]
        decision, reason = predict_rule(material_class, attr_labels)
        results_list.append((cropped, material_class, attr_labels, decision, reason))

    st.subheader("ğŸ” ë¶„ë¥˜ ê²°ê³¼")

    for idx, (img, material, attrs, decision, reason) in enumerate(results_list):
        with st.expander(f"ğŸ“¦ ê°ì²´ {idx+1} - {'âœ…' if 'ê°€ëŠ¥' in decision else 'âŒ'} {decision}", expanded=False):
            st.image(img, caption=f"ê°ì²´ {idx+1}", width=200)
            st.markdown(f"**ì¬ì§ˆ:** {material}")
            st.markdown(f"**ì†ì„±:** {', '.join(attrs) if attrs else 'ì—†ìŒ'}")
            st.markdown(f"**íŒë‹¨ ê²°ê³¼:** {'âœ…' if 'ê°€ëŠ¥' in decision else 'âŒ'} {decision}")
            st.markdown(f"**ì‚¬ìœ :** {reason}")
